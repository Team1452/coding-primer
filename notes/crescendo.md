# Crescendo

`crescendo-2024` is the codebase we used for the 2024 FRC game, "Crescendo". If you can follow [Tetris](/notes/tetris) or especially [Swerve](/notes/swerve), then FRC programming is relatively trivial. (I already ranted in the README, but the programming part of FRC *is* trivial—it's dealing with long feedback cycles and limited time with the robot that's the hard part.) The only new things you need to know about programming for FRC is how to use WPILib, the enormously useful library that handles the complicated internals of an FRC robot, and the third-party libraries provided by the electronics vendors whose products we use. For the Crescendo robot, we used a Pigeon (a fancy gyro), CANCoders, and NEO motors driven by SparkMAX motor controllers. The first two are both made by Cross the Road Electronics (CTRE) and use a shared library, and the latter two are made by REV Robotics who have their own library.

The only involved part of the codebase programming wise was implementing swerve drive, which was originally written from scratch—with a lot of the same code in the Swerve visualizer! But we ended up switching to a plug-and-play library [YAGSL ("Yet Another Generic Swerve Library")](https://yagsl.gitbook.io/yagsl)—which was somehow about as painful to get working as writing it from scratch, though ultimately YAGSL was more reliable after fidgeting with it for a while.

## [WPILib](https://docs.wpilib.org/en/stable/docs/software/what-is-wpilib.html)

As of 2024, an FRC robot uses the roboRIO from National Instruments as its brain. The roboRIO is really just an underpowered general computer (like a proprietary, more frustrating Raspberry PI) that runs a custom distribution of Linux. You could really run whatever code you want on it, but the de facto required layer to use is WPILib. WPILib does two important things. Most importantly, it provides the HAL ("Hardware Abstraction Layer") that handles the nitty gritty of doing basic things on the actual hardware so you don't have to, and on top of that it provides a library/framework for you to write your robot code around.

The documentation (https://docs.wpilib.org) is really thorough, and explain everything quite well, but generally a robot codebase looks like `Robot`, which, as almost always a subclass of something called `TimedRobot`, has some methods for doing things every tick (like in Tetris or the swerve visualizer), `RobotContainer`, which invokes `Command`s for not necessarily doing things every tick but only in response to certain events (like a button being pressed), various `Subsystem`s which each own managing some part of the robot (either literal or abstract) and custom `Command`s for simplifying `RobotContainer`. There's also usually a `Constants` file for storing all the shared constants or empirically found "magic numbers", and `Main` to be the explicit entrypoint to declare how the codebase starts (which is almost always just plainly running `Robot` through `RobotBase.startRobot`).

Part of the reason why the example projects are all games with game loops is because `Robot`/`TimedRobot` is the same thing! (This is also of course the same with Arduino or Raspberry Pi programming, if you've messed with either before.) And so you'd do things more-or-less the same way, just with a separate "game loop" for each mode (an "always on" `robotInit`/`robotPeriodic` loop, and then also `init`/`periodic` methods for each mode: disabled, autonomous, teleop, test and simulation). When a mode is selected (say when "enable" is clicked with "teleop" selected) the corresponding `[mode]Init` method is called, and then depending on the period (which is by default 20 milliseconds, though this can be easily changed) the corresponding `[mode]Periodic` method is called. So in `teleopPeriodic`, you would, for example, read the relevant joysticks (e.g. through the `XboxController` class) and update the drivebase motor controllers respectively (with a couple lines of code). Or, if you wanted to shoot a held game piece when a button is pressed, you'd do something like (`if (controller.getYButtonPressed()) shooter.shoot();`). The difference in that case between something like `getYButton()` and `getYButtonPressed()` is that the first returns `true` if the button is _held_, but the second returns `true` if and only if the button is currently held but wasn't the last time you called that function. So if you use that, then you won't spam the shooter with `shoot()` commands for every 20 milliseconds the button happens to be held down for after being pressed, but instead only call it once.

This is all simple as it is, but to make things of the latter category more explicit, it might be nicer to directly write that you want one thing to happen (shooting) as soon as something else happens (a condition is met: the Y button is pressed), instead of having a bunch of if statements or extraneous boolean variables for bookkeeping details around doing one-time things in loop. So WPILib has a ["Command-Based" API](https://docs.wpilib.org/en/stable/docs/software/commandbased/what-is-command-based.html) (in a third layer, called `wpilib2`) which does just that. Code written using `Command`s is generally more readable than code for `TimedRobot`, and has the benefit of having the control scheme/high level interface to the robot being immediate just from skimming `RobotContainer`, which is where you set up all your commands at start time.

So to illustrate with an example from the codebase, the command for shooting to a speaker/high goal looks like 

```java
new Trigger(driverXbox::getYButton)
      .toggleOnTrue(
        new InstantCommand(() -> setHoldingNote(true))
          .andThen(
            shooterSubsystem
              .createShootSpeakerCommand(intakeIndexerSubsystem)
          )
      );
```

so what it generally does might be obvious enough, but it just says that when `driverXbox.getYButton()` switches from false to true (what `Trigger` and `toggleOnTrue` do), do two things. First, call a helper function `setHoldingNote(true)`, which changes some internal state and does some other things like rumbling the controller and changing the LEDs. Then, invoke a shoot speaker command (constructed by calling a helper function on the shooter subsystem at start up). The bulk of `RobotContainer.java` is lines like these, which together outline the behavior of the robot.

Mildly more interesting are how `Subsystem` and `Command` work. Each are abstract classes that frame each pattern. A `Subsystem` is just a little `TimedRobot` for a specific thing, like managing the robot's indexer or its drivebase. A `Subsystem` is _scheduled_ by the `CommandScheduler` (done by default in the superclass constructor) meaning that every time `CommandScheduler.getInstance().run()` is called in `robotPeriodic` in `Robot`, that subsystem's `periodic()` method is called. A `Command` is, simply enough, very similar—it's like `TimedRobot`, but for carrying out some task rather than just encapsulating some robot state. When scheduled (with `.schedule()`, done whenever you want to start it) _its_ analogous periodic method `execute` is called every tick, after `initialize` is initially called for setting things up. Then, either when the command is _interrupted_ (by calling `<command>.cancel()`) or its `isFinished()` returns true, meaning the command has messaged that it's finished its work, the command is taken off the command scheduler and is no longer called. At a high level, the different `Subsystem`s of a robot define its state/variables, and the different `Command`s (sometimes constructed and returned by a subsystem for convenience) define its behavior. To this end, the last thing I'll mention is that a `Command` has a notion of _requirements_, which are the subsystems that it needs to do something on the robot. You explicitly declare these in a command constructor by calling `addRequirements(subsystem1, subsystem2, ...)`. The point of this is that you don't want two commands running at once. E.g. it doesn't make sense to have both "shoot to low goal" and "shoot to high goal" commands running at the same time, and either way both would be sending conflicting instructions to the shooter subsystem. So the `CommandScheduler` does a bit of extra work in only actually following through with scheduling commands that don't take up any subsystems currently in use.

## Overview

The codebase for this year was pretty embarassing (part of this was how much had to be written last minute, part was that I'm a bad programmer) but the structure is explained above/in the WPILib docs. We had the control scheme written out in `RobotContainer`, our five subsystems `ClimbSubsystem`/`IntakeIndexerSubsystem`/`LEDSubsystem`/`ShooterSubsystem` and `SwerveSubsystem`. The slightly funky thing I haven't mentioned is our last minute adoption of [AdvantageKit](https://github.com/Mechanical-Advantage/AdvantageKit), an extremely thorough FRC logger with a corresponding desktop interface. As a baseline, it'll log every basic hardware detail it can scour when the robot is run (which is more rigorous than what WPILib/DriverStation does by default) and on top of that you can log whatever state you interface with through an `IO` wrapper. This is explained in the documentation, but although this makes your code slightly hairier, it hooks the robot state to its powerful suite of tools and visualizers. This is what we do with the variables relating to swerve and the swerve modules (which are just thin wrappers over the YAGSL swerve library), which worked out to be critical when having to quickly troubleshoot a problem with our coordinate system in testing one of our autos.

We have an array of commands for doing various different things in `commands`. Each one is mostly self-explanatory, but I'll mention that our autos were hardcoded commands generated by an ad hoc tool, uh, slapped together on a bus ride to a tournament, which is why they're so repetitive. We were originally using a separate auton planning tool called [PathPlanner](https://pathplanner.dev/home.html) (to my knowledge the tool of choice by most teams). But, we ran into some issues that we decided to be too much of a pain to be worth pursuing, and switched to a simpler solution (the janky custom tool) instead.

### Limelights

That year we also did some mildly fancy things with vision. We had a [Limelight](https://limelightvision.io/) (a Raspberry Pi with a camera module and some custom software) on the front of the robot for detecting game pieces, and one on the back for detecting AprilTags, which are QR code-like fiducial targets plastered over well specified parts of the field that help with aligning the robot for different tasks. We would get whether the closest game piece is to the left or right of the robot's center axis from the front Limelight, and use that to turn accordingly when a button is held through the `NoteMode` command; and passively use updates from the back Limelight for both localizing the robot on the field (getting its position and rotation/pose) and also actively use it when aligning to a goal, which in the game would have associated AprilTag(s). We originally used a little OpenCV script (a standard computer vision library) for tracking orange blobs with holes (what a game piece looked like from the camera) and getting their relationships to the robot's position that way. But this was sensitive to lighting conditions, which usually weren't consistent across the field, so we ended up switching to using Deep Learning (!!1!!) and Big Data vis-a-vis plugging in a [Google Coral](https://coral.ai/), a hardware accelerator, into the Limelight over USB and loading some pretrained game piece detection model.

## Notes for future years

I apologize this file is so brief; in general the codebase is quite simple, if messier than it should be, and isn't too hard to understand with some effort and a knowledge of Java (the emphasis of this repo). But I'll spend the last few words bequething some personal takeaways from that year. My main mistake was not using the first few weeks of the build season as well as I could have—this is the time for figuring out the tools you're using (in our case, the swerve library, path planner, and logger) and ironing out basic details like making sure your coordinate system is well specified and obvious (the fact it never really was is still quite embarrassing) and your timeline/priorities. It is easy to maybe brush off the necessity of code team work when there isn't even a robot yet to test, but doing the groundwork that will make that later, limited testing time as efficient and painless as possible means these early weeks are maybe just as important. So if I put more energy in getting PathPlanner to work with our codebase, we would have had more robust and capable autos than those generated by the janky tool I wrote. Or conversely, if I spent more time writing a better, more reliable tool, that would've given us robust autos in that case too. In that sense a lot of FRC is about picking your battles. You're trying to pull something off (making a robot that solves a task) and only have so many resources (a handful of weeks, your expertise). So deciding where to put those resources is a skill in its own right.
